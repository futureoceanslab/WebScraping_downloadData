WEB SCRAPING
========================================================
author: Future Ocean Lab
date: 15/11/2017
width:1500
height: 1300

Theory for "Wreb Scraping"- Outline
========================================================
>+ Web Scraping

>+ Structure of WebPage: DOM and Nodes

>+ CSS selectors, Chrome selector gadget (and xpath)

>+ Rvest package


Wreb Scraping
========================================================

 *"Web scraping (web harvesting or web data extraction) is data scraping used for extracting data from websites"*
 
  Techniques:
+	Human copy-and-paste

+	Text pattern matching

+	HTTP programming	

+	HTML parsing

+	Vertical aggregation

+	Semantic annotation recognizing

+	Computer vision web-page analysis

+ __DOM parsing__

__By embedding a full-fledged web browser, such as the *Internet Explorer* or the *Mozilla* browser control, programs can retrieve the dynamic content generated by client-side scripts. These browser controls also parse web pages into a *DOM tree*, based on which programs can retrieve parts of the pages.__

Structucture of WebPapge
========================================================

+ [Document Object Model, DOM] (http://librosweb.es/libro/ajax/capitulo_4.html) (Document Object Model) is a group of facilities designed to manipulate XML, XHTML and HTMML documents. DOM is an [Application Programming Interfaces, API](https://hipertextual.com/archivo/2014/05/que-es-api/) that we can use to manipulate XHTML pages easly and fastly.

+ At first step, DOM transform internamente the original XML file in a easy-structure composed by hierarchy of nodes.So, DOM transfoms the XML code in several interconnected nodes tree-shaped. **Webpages can be understanded as structure of hierarchied nodes**

+ [Structure of nodes] (http://librosweb.es/libro/ajax/capitulo_4.html) The node always have the element target and text target related. 

+ Types of main nodes: 
  - Document
  - DocumentType
  - Element
  - Text
  - Atribute
  - Comment
  - CDataSection
  - Table ...

+ The functions that a DOM provides let us delete, add, modify and replace any node from any document easly (1998). 

CSS selector (chrome gadget)
========================================================

+ **CSS Selectors** Selectors such as "Chrome selector gadget" are used in CSS to select the parts of the HTML that are being styled. You can use several different methods for selecting an element (atribute, id, child...etc)

+ **Xpath** Describing the whole path ....

Rvest Package
========================================================

We will scrape with R, and for it we will use: 

>**Rvest package** rvest is new package that makes it easy to scrape (or harvest) data from html web pages, inspired by libraries like *beautiful soup*. Also you  can navigate around a website as if you're in a browse. Finally, guess_encoding() and repair_encoding() enable you to detect and repair encoding problems.  can navigate around a website as if you're in a browser


Get into Practice - Outline
========================================================
>+ R project

>+ Standard edition: Title, author, date and output

>+ Install packages and other libraries

>+ Use "Rvest" package (read url and scrap data)

>+ Build a database (dataframe) with these data

>+ Plot and use this database for our research goals


Get into Practice!
========================================================
__1.__ Create a R project with name "Web Scraping"

__2.__ Open a new R-sheet/Rmd-sheet

__3.__ Set the title, the author, date and output(html)

__i.e.__ 

+ title: "Web Scraping seminar"

+ author      : Future Oceans Lab

+ date        : 2017-11-15

+ output      : html


3,2,1.. GO!
========================================================
- Installing packages
----------------------

    install.packages("plotly")
    
    install.packages("stats")

    install.packages("xml2")
    
    install.packages("data.table")
    
    install.packages("readr")
    
    install.packages("rvest")
    
    install.packages("deplyr")
    
    install.packages("stringr")



- Calling libraries
--------------------

```r
library(xml2)
library(stats)
library(dplyr) # basic
library(rvest) # html scraping (load automatically xlm2)
library(readr) # files operations
library(data.table) #
library(stringr) # strins operations
library(plotly)
```



Let's scrape!
========================================================

We will start with a easy example of a static website

A)Who doesn't like movies?     [IMBD!] (http://www.imdb.com/movies-coming-soon/?ref_=nv_mv_cs_4)

                                    STEPS TO FOLLOW


__1st. Read the html__


```r
movie<-read_html("http://www.imdb.com/movies-coming-soon/?ref_=nv_mv_cs_4")
```

__2nd.Use CSS selectors and/or xpath__


__3rd.Think about the inputs we want and get them__ (*TITLE*,*TIME*, *GENRE* and *IMAGE*)

*TITLE*

```r
title<-movie%>%
  html_nodes("td h4")%>%
html_text()
```

*TIME*

```r
time.min<- movie%>%
  html_nodes("p time")%>%
html_text()
```

========================================================

*GENRE*

```r
genre<- movie%>%
  html_nodes("time+span")%>%
  html_text()
```

*IMAGE*

```r
##attribute: link and image
image<- movie%>%
  html_nodes(".shadowed")%>%
  html_attr("src")
```

__4th.Build a dataset with our data__ (data.frame/data.table)

```r
movie_table<-data.frame(title,genre,time.min,image) 
```

__5th. Remove "min" of time-variable, and make it numeric__ (functional for future analysis)

```r
movie_table$time.min<-gsub("min","",movie_table$time)
movie_table$time.min<-as.numeric(movie_table$time)
```

========================================================

__6th. Pull some answers from our data__

```r
##Movies that last > or < 50 min. 
movie_table$title[movie_table$time.min>50]
```

```
[1]  Coco (2017)                            
[2]  El instante más oscuro (2017)          
[3]  Call Me by Your Name (2017)            
[4]  The Man Who Invented Christmas (2017)  
[5]  Chappaquiddick (2017)                  
[6]  Bombshell: The Hedy Lamarr Story (2017)
6 Levels:  Bombshell: The Hedy Lamarr Story (2017) ...
```

```r
##Movies that last  > or < 50 min. 
movie_table$title[movie_table$time.min<50]
```

```
factor(0)
6 Levels:  Bombshell: The Hedy Lamarr Story (2017) ...
```

========================================================

__6th. Pull some answers from our data__

```r
##Movies that last > or < 50 min with a specific genre.
movie_table$title[movie_table$genre!="Action" & movie_table$time.min>50]
```

```
[1]  Coco (2017)                            
[2]  El instante más oscuro (2017)          
[3]  Call Me by Your Name (2017)            
[4]  The Man Who Invented Christmas (2017)  
[5]  Chappaquiddick (2017)                  
[6]  Bombshell: The Hedy Lamarr Story (2017)
6 Levels:  Bombshell: The Hedy Lamarr Story (2017) ...
```

__7th. Plotting our data__

i.e.

*Barplot*

>__movietime_genre__<-by(movie_table$time.min, genre, function(x) mean(x))
barplot(movietime_genre,main="Average movie time per genre",xlab="Genre",ylab="Time(minutes)")


*Interactives plot*

>__plot_ly__(data=movie_table,x=genre, y=time, type="bar", marker=list(color="thistle1"),
        hovertext="text", text=title)%>%layout(title="Movie times by genre",color="thistle1")



Climate in other formats
========================================================

Once we know a bit about the workflow of scraping, let's see how it is with other formats

B) How can we understand Adaptation to CLimate Change if we don't know the climate... 
[METEOGALICIA!] (http://www.meteogalicia.gal/web/index.action)

As you already know... we can scrape the url and take text and image data from it, but how it works with tables?

                                   TABLES

```r
#Read HTML METEOGALICIA
weather<- read_html("http://www.meteogalicia.gal/web/index.action")
# Use CSS selectors
# Think about the input (TABLE), and take it.
table<-html_node(weather,".temperatrasTabla")%>%
  html_table()
table 
```

Adaptation of environment under Climate Change scenario also includes SOCIAL DIMENSION!
So, who are these fishers? [PESCADEGALICIA!] (http://confrariasgalicia.org/afiliados/)


```r
#Read HTML PESCADEGALICIA
Confrarias<- read_html("http://confrariasgalicia.org/afiliados/")
# Use CSS selectors
# Think about the input (TABLE), and take it.
af<-html_node(Confrarias,"table")%>%
  html_table() # I don't know how to choose the different ones...
```

========================================================


```r
 af
```

```
          X1       X2      X3         X4       X5      X6         X7
1 FEDERACION EMPRESAS % TOTAL % EMPRESAS TRABALL. % TOTAL % TRABALL.
2   A CORUÑA    2.143  16,83%     49,70%    4.306  33,81%     51,13%
3       LUGO      146   1,15%      3,39%      652   5,12%      7,74%
4 PONTEVEDRA    2.023  15,89%     46,92%    3.464  27,00%     41,13%
5      TOTAL    4.312  33,86%       100%    8.422  66,14%       100%
         X8      X9
1 AFILIADOS % TOTAL
2     6.449  50,64%
3       798   6,27%
4     5.487  43,09%
5    12.734    100%
```

```r
 af[2]
```

```
        X2
1 EMPRESAS
2    2.143
3      146
4    2.023
5    4.312
```

```r
 af[5]
```

```
        X5
1 TRABALL.
2    4.306
3      652
4    3.464
5    8.422
```


 THE RING OF FIRE of our seminar!
========================================================
We have already learnt about scraping different types of data from static webpages but what happen with dinamyc ones?
Let see how it works...

C) Who doesn't need a wollen jersey for winter? 
[KATIA] (http://www.katia.com/ES/lanas.html)

__1st. Read the html__

```r
wool<-read_html("http://www.katia.com/ES/lanas.html")
```

__2nd. Use CSS selectors__

__3rd. Think about the inputs we want and get them__ (*NAME*, *PRICE* and *IMAGE*)

Oops! There are so many links....

So first, we need to list all these links for scrape them later on.


```r
products<-wool%>%  
        html_nodes("#principal .txt_normal .txt_normal")%>% 
        html_text() 

links<- wool%>%html_nodes('#principal .txt_normal .txt_normal')%>%
        html_attr("href") 
```

__4th. Build a temporal table with our data__ (data.frame/data.table)

```r
Table_links<-data.table(products,links)
```

========================================================


```r
Table_links
```

```
                     products
  1:      AIR ALPACA - 2.75 
  2:         AIR LUX - 5.90 
  3:            AIRE - 5.60 
  4:          ALASKA - 2.99 
  5:   ALPACA SILVER - 4.95 
 ---                         
118:           VERSO - 3.95 
119:          WARMY - 19.95 
120: WINTER RAINBOW - 21.50 
121:    WINTER WASHI - 7.50 
122:         X-TREME - 9.95 
                                                                                               links
  1:            https://www.katia.com/ES/lanas-otoño-invierno-airalpaca.html?idLana=airalpaca&lng=ES
  2:                  https://www.katia.com/ES/lanas-otoño-invierno-airlux.html?idLana=airlux&lng=ES
  3:                      https://www.katia.com/ES/lanas-otoño-invierno-aire.html?idLana=aire&lng=ES
  4:                  https://www.katia.com/ES/lanas-otoño-invierno-alaska.html?idLana=alaska&lng=ES
  5:      https://www.katia.com/ES/lanas-otoño-invierno-alpacasilver.html?idLana=alpacasilver&lng=ES
 ---                                                                                                
118:                    https://www.katia.com/ES/lanas-otoño-invierno-verso.html?idLana=verso&lng=ES
119:                    https://www.katia.com/ES/lanas-otoño-invierno-warmy.html?idLana=warmy&lng=ES
120:    https://www.katia.com/ES/lanas-otoño-invierno-winterrainbow.html?idLana=winterrainbow&lng=ES
121:        https://www.katia.com/ES/lanas-otoño-invierno-winterwashi.html?idLana=winterwashi&lng=ES
122: https://www.katia.com/ES/lanas-otoño-invierno-primavera-verano-xtreme.html?idLana=xtreme&lng=ES
```

========================================================

Once we have a temporal table with our data, we start to scrape the links over this table...by a function...

__5th. Hhow to create a function which "re-do"" 1st-4th steps__ (Read url, use CSS, think the inputs, build the temporal tableII with our data, into a function.


```r
scrape_katia <- function(url_to_scrape) {
        
        scraped_url <- read_html(url_to_scrape) # very important, read only once
        price <- scraped_url %>%
                html_nodes("#principal b") %>% html_text()
        nome<-scraped_url%>%
                html_nodes(".titol_modrev_mini_new")
        name<-name[1]%>%html_text()%>%
                str_replace(price,"")# Exemplo de acceso por codigo, non por CSS Selectors
       images<-scraped_url %>%
        html_nodes("#gal1 .llistatcolors")%>%html_attr("data-image")
       
        Sys.sleep(1) # important, without it, a DOS can be caused
      
      temp_table<-data.table(name,price,images) 
      # final_table<<-rbindlist(list(final_table,temp_table))
        return(temp_table)# return the table which we created before
}
final_table<-lapply(Table_links$links_wool,scrape_katia) #lappy, take a common of vectors and apply them to a function. Produce a list, take it easy, it runs very slow....
final_table<-Reduce("rbind",final_table) # conversion of the list in a table
write.csv2(final_table,file = "final_table.csv")
```

Explanation of last steps
========================================================


__6th. Build a temporal table with all the data__ : table I(1st interface of the webpage) and II(2nd interface of webpage).

When we have created the function, we use "lapply" to apply the function over the data.table (data.frame) that we want. We can see here a very similar process of a "loop". 

```r
final_table<-lapply(Table_links$links_wool,scrape_katia) #lappy, take a common of vectors and apply them to a function. Produce a list, take it easy, run very slow....
```



__7th.Create the final dataframe and edit it in csv format__

We use "reduce" to call the "rbind" function and apply it on "final_table", by this way we create a dataframe with the final data.
Finally, we write it in csv format.

```r
final_table<-Reduce("rbind",final_table) # conversion of the list in a table. Reduce = "do.call""
write.csv2(final_table,file = "final_table.csv")
```


So  far ...  this is all, folks!

                           Thanks for attending!



